# Бинарная классификация текстов
## Данные
Предоставлен размеченный корпус текстов на английской языке - всего чуть менее 160К объектов, около 10% были помечены как токсичные.
## Задача
Построить модель определения токсичности текстов со значением метрики F1 на тестовом наборе не менее 0,75.
## Описание
Проанализирован набор данных с разметкой о токсичности правок. Проведено сравнение разных техник токенизации. Целевых показателей метрики F1 я сумел добиться при использовании tf-idf векторизации и логистической регрессии, tf-idf векторизации и градиентного бустинга (lightGBM), BERT embeddings и логистической регрессии.
## Использованные библиотеки
pandas, numpy, scipy, matplotlib.pyplot, time, re, nltk, sklearn, lightgbm, torch, transformers, tqdm.notebook
