{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо обучить модель, способную классифицировать комментарии на позитивные и негативные. Для обучения предоставлен набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Необходимо построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeibovdei/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "# from pymystem3 import Mystem\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "# import spacy\n",
    "import lightgbm as lgbm\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sergeibovdei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sergeibovdei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sergeibovdei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sergeibovdei/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее кол-во комментариев:  159571\n",
      "Доля токсичных комментариев: 10.17%\n"
     ]
    }
   ],
   "source": [
    "print('Общее кол-во комментариев: ', len(df_comments))\n",
    "print(f'Доля токсичных комментариев: {df_comments[\"toxic\"].mean():6.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы после ознакомления с данными:\n",
    "- Предоставлено почти 160 тысяч текстов на английском языке. Сразу видно, что каждый текст содержит не одно предложение, есть служебные символы (например, \\n) и почти наверняка различные сокращения, жаргонизмы.\n",
    "- Доля токсичных комментариев небольшая - всего 10%, что означает существенную несбалансированность нашего корпуса текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка лемм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После ознакомления с возможностями библиотеки nltk представляется несколько возможных вариантов преобразования английского текста к набору лемм:\n",
    "\n",
    "- На самом последнем этапе подготовки данных мы будем лемматизировать английские слова  при помощи Wordnet лемматизатора в nltk.\n",
    "- Лемматизация может производиться с использованием разметки по частям речи (pos-tag), и это должно быть важным обстоятельством, поскольку без pos-tag лемматизатор nltk может и не преобразовать слово к лемме, если у разных частей речи разные леммы. Разметка pos_tag процесс длительный, тем не менее было принято решение проверить, как этап pos-тэгирования повлияет на точность модели. Соответственно, при обучении (и сравнении) моделей будут использоваться наборы лемм, полученных с pos-tag и без них.\n",
    "- Наконец, очистка текста. На этапе очистки (а это первый этап предобработки) каждый текст и так разделяется на отдельные слова (токены), но это разделение не равносильно токенизатору на отдельные слова при помощи nltk. При обучении (и сравнении) моделей будут использоваться наборы лемм, в которых токенизация производилась при помощи nltk и при помощи обычного split по пробелам.\n",
    "- Сформируем 2 набора лемм, варьируя обозначенные выше варианты предобработки. На предварительном этапе работы над проектом было сформировано 4 набора лемм и проведено исследование моделей линейной регрессии с каждым. Поскольку процесс лемматизации небыстрый, было принято решение оставить только два набора - достаточно для сравнения наборов лемм, но не требует такого кол-ва ресурсов:\n",
    "  - nltk токенизация без очистки, затем лемматизация с pos-tag\n",
    "  - очистка и токенизация просто по пробелам, затем лемматизация без pos-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9 \\']', ' ', text).split()\n",
    "\n",
    "# данная функция необходима для лемматизации при преобразовании pos_tag nltk в pos_tag wordnet\n",
    "def tag_wordnet(tag_nltk):\n",
    "    if tag_nltk[0] == 'J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag_nltk[0] == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif tag_nltk[0] == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в этом словаре мы сохраним полученные наборы лемм\n",
    "lemmas_dict = {}\n",
    "# и время для подготовки таких лемм\n",
    "lemmas_time_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 39s, sys: 11.8 s, total: 11min 51s\n",
      "Wall time: 12min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Here we create lemmatized tokens using Wordnet tokenizer and pos-tag\"\"\"\n",
    "\n",
    "dict_key = 'no clearing, tokenization, lemmatization WITH pos-tag'\n",
    "start_time = time.time()\n",
    "word_list = df_comments['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# create Series of lists of tuples (token, nltk.pos_tag)\n",
    "word_list_nltk_tagged = word_list.apply(nltk.pos_tag)\n",
    "\n",
    "# mapping nltk pos_tag to wordnet.pos_tag\n",
    "word_list_wordnet_tagged = word_list_nltk_tagged.apply(lambda x: [(w[0], tag_wordnet(w[1])) for w in x])\n",
    "\n",
    "# create Series of lists of lemmas\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_list = word_list_wordnet_tagged.apply(lambda x: [lemmatizer.lemmatize(w[0], w[1]) for w in x])\n",
    "\n",
    "# create Series of strings of lemmas separated with space\n",
    "lemmas_dict[dict_key] = lemma_list.apply(lambda x: ' '.join(x))\n",
    "end_time = time.time()\n",
    "\n",
    "lemmas_time_dict[dict_key] = round(end_time-start_time, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.5 s, sys: 4.11 s, total: 54.6 s\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Here we clear at first (and split to tokens) then lemmatize not using pos-tag\"\"\"\n",
    "\n",
    "dict_key = \"clearing, splitting, lemmatization WITHOUT pos-tag\"\n",
    "start_time = time.time()\n",
    "\n",
    "# clear and split to tokens\n",
    "word_list = df_comments['text'].apply(clear_text)\n",
    "\n",
    "# create Series of lists of lemmas\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_list = word_list.apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "\n",
    "# create Series of strings of lemmas separated with space\n",
    "lemmas_dict[dict_key] = lemma_list.apply(lambda x: ' '.join(x))\n",
    "end_time = time.time()\n",
    "\n",
    "lemmas_time_dict[dict_key] = round(end_time-start_time, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "____________________________________________________________________________________________________\n",
      "no clearing, tokenization, lemmatization WITH pos-tag :\n",
      "Explanation Why the edits make under my username Hardcore Metallica Fan be revert ? They be n't vandalisms , just closure on some GAs after I vote at New York Dolls FAC . And please do n't remove the template from the talk page since I 'm retired now.89.205.38.27\n",
      "____________________________________________________________________________________________________\n",
      "clearing, splitting, lemmatization WITHOUT pos-tag :\n",
      "Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They weren't vandalism just closure on some GAs after I voted at New York Dolls FAC And please don't remove the template from the talk page since I'm retired now 89 205 38 27\n"
     ]
    }
   ],
   "source": [
    "# сравним результаты на каком-то примере\n",
    "text_id = 0\n",
    "print('raw text:')\n",
    "print(df_comments.loc[text_id, 'text'])\n",
    "for lemma in lemmas_dict:\n",
    "    print(''.join(['_'] * 100))\n",
    "    print(lemma, ':')\n",
    "    print(lemmas_dict[lemma][text_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация без pos-tag и c pos-tag заметна невооруженным глазом (were - be). Проверим, как это скажется на моделях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ограничение на длину текста: 2500, кол-во таких текстов:     156733, доля токсичных текстов: 10.16%\n",
      "Ограничение на длину текста: 1000, кол-во таких текстов:     146187, доля токсичных текстов: 10.52%\n",
      "Ограничение на длину текста:  500, кол-во таких текстов:     125544, доля токсичных текстов: 11.26%\n",
      "Ограничение на длину текста:  200, кол-во таких текстов:      78186, доля токсичных текстов: 13.48%\n",
      "Ограничение на длину текста:  100, кол-во таких текстов:      41575, доля токсичных текстов: 16.02%\n",
      "Ограничение на длину текста:   50, кол-во таких текстов:      17324, доля токсичных текстов: 17.46%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"При построении модели с использованием BERT было замечено интересное свойство предоставленного корпуса текстов - \n",
    "если ограничивать длину текста, доля токсичных текстов увеличивается. Ниже продемонстрирована такая зависимость:\"\"\"\n",
    "\n",
    "text_sizes = df_comments['text'].apply(len)\n",
    "\n",
    "for length in [2500, 1000, 500, 200, 100, 50]:\n",
    "    df_limited_length = df_comments[text_sizes < length]\n",
    "    toxic_share = df_limited_length['toxic'].mean()\n",
    "    print(f'Ограничение на длину текста: {length:4}, кол-во таких текстов: {len(df_limited_length): 10}, доля токсичных текстов: {toxic_share:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся этим фактом при построении моделей - добавим в качестве признака длину текста в дополнение к вектору текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение с использованием векторизации tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Будет проведено исследование моделей для разных наборов лемм. \n",
    "- Векторизация лемм будет производиться методом TF-IDF. Это означает необходимость проведения векторизации уже после разделения выборки на обучающую и тестовую.\n",
    "- Будет проведено сравнение моделей логистической регрессии и градиентного бустинга, с подбором гиперпараметров для достижения поставленной цели по метрике F1 = 0.75.\n",
    "- Будет испробована техника upsampling для балансировки классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_for = {\n",
    "    'F1_test'     : 'F1 on test',\n",
    "    'F1_valid'    : 'F1 on valid',\n",
    "    'F1_train'    : 'F1 on train',\n",
    "    'Accuracy'    : 'Accuracy on test',\n",
    "    'time_fit'    : 'time to fit',\n",
    "    'time_predict': 'time to predict',\n",
    "    'time_lemma'  : 'time to vectorize',\n",
    "    'ratio'       : 'possible overfitting'\n",
    "}\n",
    "all_results = pd.DataFrame(columns=[name_for[x] for x in name_for])\n",
    "targets = df_comments['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 134\n",
    "def fit_and_test(X_train, X_test, y_train, y_test, lemmas_key, feature_processing_describe, model_method, **kw_model):\n",
    "    start_fit = time.time()\n",
    "    model = model_method(**kw_model)\n",
    "    model.fit(X_train, y_train)\n",
    "    end_fit = time.time()\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    end_predict = time.time()\n",
    "    model_name = model.__class__.__name__ + ', ' + ', '.join([f'{i}={kw_model[i]}' for i in kw_model])\n",
    "    index_row = model_name + '. Features preparing: ' + lemmas_key + feature_processing_describe\n",
    "    f1_test = round(f1_score(y_test, y_test_predict), 3)\n",
    "    f1_train = round(f1_score(y_train, y_train_predict), 3)\n",
    "    all_results.loc[index_row, name_for['F1_test']] = f1_test\n",
    "    all_results.loc[index_row, name_for['F1_train']] = f1_train\n",
    "    all_results.loc[index_row, name_for['Accuracy']] = round(accuracy_score(y_test, y_test_predict), 3)\n",
    "    all_results.loc[index_row, name_for['time_fit']] = round(end_fit - start_fit, 3)\n",
    "    all_results.loc[index_row, name_for['time_predict']] = round(end_predict - end_fit, 3)\n",
    "    all_results.loc[index_row, name_for['time_lemma']] = lemmas_time_dict[lemmas_key]\n",
    "    all_results.loc[index_row, name_for['ratio']] = round(f1_train / f1_test,3)\n",
    "\n",
    "def tfidf_features_split(lemmas_key, targets, test_size):\n",
    "    lemmas_train, lemmas_test, y_train, y_test = train_test_split(lemmas_dict[lemmas_key], targets, random_state=RANDOM_STATE, test_size=test_size)\n",
    "    tf_idf = TfidfVectorizer(stop_words=stopwords).fit(lemmas_train)\n",
    "    X_train = tf_idf.transform(lemmas_train)\n",
    "    X_test = tf_idf.transform(lemmas_test)\n",
    "    feature_processing_describe = ', TfidfVectorizer'\n",
    "    return X_train, X_test, y_train, y_test, feature_processing_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим отдельно функцию которая при создании X_ добавляет длину текста в качестве еще одного обучающего признака \n",
    "def column_stack_sparse_matrix(X_sparse, array_1d_to_add):\n",
    "    add_to_X = csr_matrix(array_1d_to_add).T\n",
    "    return hstack((X_sparse, add_to_X))\n",
    "\n",
    "def tfidf_features_length_split(lemmas_key, targets, test_size):\n",
    "    lengths = lemmas_dict[lemmas_key].apply(len).values\n",
    "    lemmas_train, lemmas_test, lengths_train, lengths_test, y_train, y_test = train_test_split(lemmas_dict[lemmas_key], lengths, targets, random_state=RANDOM_STATE, test_size=test_size)\n",
    "    tf_idf = TfidfVectorizer(stop_words=stopwords).fit(lemmas_train)\n",
    "    # теперь сформируем обучающие признаки, как tf_idf вектор + длина текста\n",
    "    X_train = tf_idf.transform(lemmas_train)\n",
    "    X_train = column_stack_sparse_matrix(X_train, lengths_train)\n",
    "    X_test = tf_idf.transform(lemmas_test)\n",
    "    X_test = column_stack_sparse_matrix(X_test, lengths_test)\n",
    "    feature_processing_describe = ', TfidfVectorizer + text_length'\n",
    "    return X_train, X_test, y_train, y_test, feature_processing_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Логистическая регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем логистическую регрессию на разных наборах лемм со след. гиперпараметрами:\n",
    "test_size = 0.25\n",
    "n_iter = 1000\n",
    "class_w = 'balanced'\n",
    "solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.7 s, sys: 1.05 s, total: 38.7 s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# в цикле формируем сеты данных для лог. регрессии на основе различных наборов лемм, затем обучаем и тестируем модель\n",
    "# TfidfVectorizer применяем только на train данных уже после разделения лемм на обучающие и тестировочные\n",
    "for key in lemmas_dict:\n",
    "    X_train, X_test, y_train, y_test, describe_str = tfidf_features_split(key, targets, test_size)\n",
    "    fit_and_test(X_train, X_test, y_train, y_test, key, describe_str, LogisticRegression, max_iter=n_iter, class_weight=class_w, solver=solver, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь посмотрим, изменится ли качество модели при добавлении длины текста\n",
    "for key in lemmas_dict:\n",
    "    X_train, X_test, y_train, y_test, describe_str = tfidf_features_length_split(key, targets, test_size)\n",
    "    fit_and_test(X_train, X_test, y_train, y_test, key, describe_str, LogisticRegression, max_iter=n_iter, class_weight=class_w, solver=solver, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат несколько неожиданный - самая простая (и быстрая) предобработка позволила получить более высокий результат F1 на тестовой выборке. \n",
    "- Добавление длины текста улучшило результат, но не сильно.\n",
    "- Далее попробуем улучшить результаты логистической регрессии, увеличив кол-во данных для '1' класса или уменьшив кол-во данных для '0' класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_classes(comments, targets):\n",
    "    filter_zero = (targets == 0)\n",
    "    filter_one = (targets == 1)\n",
    "    comments_positive = comments[filter_zero]\n",
    "    targets_positive = targets[filter_zero]\n",
    "    comments_negative = comments[filter_one]\n",
    "    targets_negative = targets[filter_one]\n",
    "    return comments_positive, comments_negative, targets_positive, targets_negative\n",
    "\n",
    "def upsampling(all_comments, targets, repeat):\n",
    "    comments_p, comments_n, targets_p, targets_n = split_classes(all_comments, targets)\n",
    "    comments_upsample = pd.concat([comments_p] + repeat * [comments_n])\n",
    "    targets_upsample = pd.concat([targets_p] + repeat * [targets_n])\n",
    "    return shuffle(comments_upsample, targets_upsample, random_state = RANDOM_STATE)\n",
    "\n",
    "def downsampling(all_comments, targets, fraction):\n",
    "    comments_p, comments_n, targets_p, targets_n = split_classes(all_comments, targets)\n",
    "    comments_p = comments_p.sample(frac=fraction, random_state = RANDOM_STATE)\n",
    "    targets_p = targets_p.sample(frac=fraction, random_state = RANDOM_STATE)\n",
    "    comments_downsample = pd.concat([comments_p, comments_n])\n",
    "    targets_downsample = pd.concat([targets_p, targets_n])\n",
    "    return shuffle(comments_downsample, targets_downsample, random_state = RANDOM_STATE)\n",
    "\n",
    "def tfidf_after_splitting_balancing(lemmas_key, targets, sampling_mult, test_size):\n",
    "    lemmas_train, lemmas_test, y_train, y_test = train_test_split(lemmas_dict[lemmas_key], targets, random_state=RANDOM_STATE, test_size=test_size)\n",
    "    if sampling_mult > 1:\n",
    "        feature_processing_describe = f', upsampling x {sampling_mult}, TfidfVectorizer'\n",
    "        lemmas_train_balanced, y_train_balanced = upsampling(lemmas_train, y_train, sampling_mult)\n",
    "    else:\n",
    "        feature_processing_describe = f', downsampling x {sampling_mult}, TfidfVectorizer'\n",
    "        lemmas_train_balanced, y_train_balanced = downsampling(lemmas_train, y_train, sampling_mult)\n",
    "    tf_idf = TfidfVectorizer(stop_words=stopwords).fit(lemmas_train_balanced)\n",
    "    X_train_balanced = tf_idf.transform(lemmas_train_balanced)\n",
    "    X_test = tf_idf.transform(lemmas_test)\n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test, feature_processing_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_key = 'clearing, splitting, lemmatization WITHOUT pos-tag'\n",
    "for params in [(9, None), (4, 'balanced'), (0.5, 'balanced')]:\n",
    "    X_train, X_test, y_train, y_test, describe_str = tfidf_after_splitting_balancing(lemmas_key, targets, params[0], test_size)\n",
    "    fit_and_test(X_train, X_test, y_train, y_test, lemmas_key, describe_str, LogisticRegression, max_iter=n_iter, class_weight=params[1], solver=solver, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=None, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 9, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.026</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 4, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.266</td>\n",
       "      <td>0.028</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, downsampling x 0.5, TfidfVectorizer</th>\n",
       "      <td>0.732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.732         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.973   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.946   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.886   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.936   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.885   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.266   \n",
       "LogisticRegression, max_iter=1000, class_weight...           1   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.026   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.028   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.012   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.301  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.265  \n",
       "LogisticRegression, max_iter=1000, class_weight...                 1.21  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Выводы.\n",
    "- Методом логистической регрессии формально удалось достигнуть требуемого качества по метрике F1. Лучший результат на тестовой выборке 0,752.\n",
    "- Результат не сильно изменился при добавлении к tf_idf векторам еще и длины текста, тем не менее с таким дополнительным обучающим признаком модель продемонстрировала более высокий  F1 на тестовой выборке и меньшую степень переобученности.\n",
    "- Upsampling/downsampling не улучшили результата F1, но увеличили показатель возможной переобученности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели LightGBM будем использовать лучший набор лемм, выявленный на этапе тестирования моделей логистической регресии, к обучающим признакам (векторизация tf-idf) будем добавлять длину текста. В отличие от процесса обучения модели логистической регрессии нам потребуется разбиение на обучающую, валидационную и тестовую выборки. Связано это с тем, что модель LightGBM  можно обучать одновременно с контролем logloss на валидационной выборке и не допустить переобученности, прекратив дальнейшие интерации (увеличение кол-ва деревьев), если logloss перестал уменьшаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в связи с изменением методики обучения модифицируем нашу функцию обучения и измерения F1\n",
    "def fit_valid_and_test(X_train, X_valid, X_test, y_train, y_valid, y_test, lemmas_key, feature_processing_describe, model_method, **kw_model):\n",
    "    start_fit = time.time()\n",
    "    model = model_method(**kw_model)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=5, verbose=100)\n",
    "    end_fit = time.time()\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    y_valid_predict = model.predict(X_valid)\n",
    "    end_predict = time.time()\n",
    "    model_name = model.__class__.__name__ + f', best iteration {model.best_iteration_}, ' + ', '.join([f'{i}={kw_model[i]}' for i in kw_model])\n",
    "    index_row = model_name + '. Features preparing: ' + lemmas_key + feature_processing_describe\n",
    "    f1_test = round(f1_score(y_test, y_test_predict), 3)\n",
    "    f1_train = round(f1_score(y_train, y_train_predict), 3)\n",
    "    all_results.loc[index_row, name_for['F1_test']] = f1_test\n",
    "    all_results.loc[index_row, name_for['F1_valid']] = round(f1_score(y_valid, y_valid_predict), 3)\n",
    "    all_results.loc[index_row, name_for['F1_train']] = f1_train\n",
    "    all_results.loc[index_row, name_for['Accuracy']] = round(accuracy_score(y_test, y_test_predict), 3)\n",
    "    all_results.loc[index_row, name_for['time_fit']] = round(end_fit - start_fit, 3)\n",
    "    all_results.loc[index_row, name_for['time_predict']] = round(end_predict - end_fit, 3)\n",
    "    all_results.loc[index_row, name_for['time_lemma']] = lemmas_time_dict[lemmas_key]\n",
    "    all_results.loc[index_row, name_for['ratio']] = round(f1_train / f1_test,3)\n",
    "\n",
    "\n",
    "# в связи с разбиением на три выборки также модифицируем функции разбиения на выборки и векторизации\n",
    "def tfidf_features_split_with_valid(lemmas_key, targets, valid_size):\n",
    "    lengths = lemmas_dict[lemmas_key].apply(len).values\n",
    "    lemmas_train, lemmas_test, lengths_train, lengths_test, y_train, y_test = train_test_split(lemmas_dict[lemmas_key], lengths, targets, random_state=RANDOM_STATE, test_size=(1 - valid_size))\n",
    "    lemmas_valid, lemmas_test, lengths_valid, lengths_test, y_valid, y_test = train_test_split(lemmas_test, lengths_test, y_test, random_state=RANDOM_STATE, test_size=0.5)\n",
    "    tf_idf = TfidfVectorizer(stop_words=stopwords).fit(lemmas_train)\n",
    "    X_train = tf_idf.transform(lemmas_train)\n",
    "    X_train = column_stack_sparse_matrix(X_train, lengths_train)\n",
    "    X_valid = tf_idf.transform(lemmas_valid)\n",
    "    X_valid = column_stack_sparse_matrix(X_valid, lengths_valid)\n",
    "    X_test = tf_idf.transform(lemmas_test)\n",
    "    X_test = column_stack_sparse_matrix(X_test, lengths_test)\n",
    "    feature_processing_describe = ', TfidfVectorizer + text_length, train+valid+test'\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test, feature_processing_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для выбора модели LightGBM будем использовать лучший набор лемм, \n",
    "# выявленный на этапе тестирования моделей логистической регресии\n",
    "lemmas_key = 'clearing, splitting, lemmatization WITHOUT pos-tag'\n",
    "valid_size = 0.6\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test, describe_str = tfidf_features_split_with_valid(lemmas_key, targets, valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.127117\n",
      "[200]\tvalid_0's binary_logloss: 0.119481\n",
      "Early stopping, best iteration is:\n",
      "[274]\tvalid_0's binary_logloss: 0.117806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeibovdei/opt/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:497: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.117844\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's binary_logloss: 0.116228\n"
     ]
    }
   ],
   "source": [
    "n_estimators_max = 1000\n",
    "for max_leaves in [31, 70]:\n",
    "    fit_valid_and_test(X_train, X_valid, X_test, y_train, y_valid, y_test, lemmas_key, describe_str, lgbm.LGBMClassifier, n_estimators=n_estimators_max, num_leaves=max_leaves, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=None, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 9, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.026</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 4, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.266</td>\n",
       "      <td>0.028</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, downsampling x 0.5, TfidfVectorizer</th>\n",
       "      <td>0.732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 274, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.759</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.957</td>\n",
       "      <td>106.455</td>\n",
       "      <td>15.418</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 158, n_estimators=1000, num_leaves=70, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.957</td>\n",
       "      <td>136.198</td>\n",
       "      <td>20.005</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.732         NaN   \n",
       "LGBMClassifier, best iteration 274, n_estimator...      0.759       0.773   \n",
       "LGBMClassifier, best iteration 158, n_estimator...      0.756       0.776   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.973   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.946   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.886   \n",
       "LGBMClassifier, best iteration 274, n_estimator...       0.849   \n",
       "LGBMClassifier, best iteration 158, n_estimator...       0.875   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.936   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            0.957   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            0.957   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.885   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.266   \n",
       "LogisticRegression, max_iter=1000, class_weight...           1   \n",
       "LGBMClassifier, best iteration 274, n_estimator...     106.455   \n",
       "LGBMClassifier, best iteration 158, n_estimator...     136.198   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.026   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.028   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.012   \n",
       "LGBMClassifier, best iteration 274, n_estimator...          15.418   \n",
       "LGBMClassifier, best iteration 158, n_estimator...          20.005   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            57.207   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            57.207   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.301  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.265  \n",
       "LogisticRegression, max_iter=1000, class_weight...                 1.21  \n",
       "LGBMClassifier, best iteration 274, n_estimator...                1.119  \n",
       "LGBMClassifier, best iteration 158, n_estimator...                1.157  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeibovdei/opt/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:497: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=None, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 9, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.026</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 4, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.266</td>\n",
       "      <td>0.028</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, downsampling x 0.5, TfidfVectorizer</th>\n",
       "      <td>0.732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 274, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.759</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.957</td>\n",
       "      <td>106.455</td>\n",
       "      <td>15.418</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 158, n_estimators=1000, num_leaves=70, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.957</td>\n",
       "      <td>136.198</td>\n",
       "      <td>20.005</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, n_estimators=274, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.96</td>\n",
       "      <td>142.534</td>\n",
       "      <td>17.193</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.732         NaN   \n",
       "LGBMClassifier, best iteration 274, n_estimator...      0.759       0.773   \n",
       "LGBMClassifier, best iteration 158, n_estimator...      0.756       0.776   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...      0.774         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.973   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.946   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.886   \n",
       "LGBMClassifier, best iteration 274, n_estimator...       0.849   \n",
       "LGBMClassifier, best iteration 158, n_estimator...       0.875   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...       0.838   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.936   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            0.957   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            0.957   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...             0.96   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.885   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.266   \n",
       "LogisticRegression, max_iter=1000, class_weight...           1   \n",
       "LGBMClassifier, best iteration 274, n_estimator...     106.455   \n",
       "LGBMClassifier, best iteration 158, n_estimator...     136.198   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...     142.534   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.026   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.028   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.012   \n",
       "LGBMClassifier, best iteration 274, n_estimator...          15.418   \n",
       "LGBMClassifier, best iteration 158, n_estimator...          20.005   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...          17.193   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            57.207   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            57.207   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...            57.207   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.301  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.265  \n",
       "LogisticRegression, max_iter=1000, class_weight...                 1.21  \n",
       "LGBMClassifier, best iteration 274, n_estimator...                1.119  \n",
       "LGBMClassifier, best iteration 158, n_estimator...                1.157  \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...                1.083  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# увеличение максимального кол-ва листьев по сравнению с дефолтным практически не улучшает качество модели, \n",
    "# но увеличивает риск переобученности - зафиксируем лучшие параметры модели LGBMClassifier\n",
    "# и переобучим модель на более общей выборке (разобьем весь сет на обучающий и тестовый)\n",
    "n_estimators_best = 274\n",
    "num_leaves_best = 31\n",
    "X_train, X_test, y_train, y_test, describe_str = tfidf_features_length_split(key, targets, test_size)\n",
    "# print('Lemmas splitted, features vectorized')\n",
    "fit_and_test(X_train, X_test, y_train, y_test, lemmas_key, describe_str, lgbm.LGBMClassifier, n_estimators=n_estimators_best, num_leaves=num_leaves_best, random_state=RANDOM_STATE)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Выводы.\n",
    "- Для обучения модели LightGBM использовался лучший набор лемм, выявленный на этапе тестирования моделей логистической регресии, к обучающим признакам (векторизация tf-idf) была добавлена длина текста. \n",
    "- Лучшая найденная модель LightGBM продемнострировала наиболее высокий результат по метрике F1 = 0.774 и мЕньшую степень возможной переобученности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Обучение с использованием BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Расчет имбедингов - можно их не рассчитывать, а загрузить из файла. Тогда следует сразу перейти к пункту 2.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициируем BERT tokenizer\n",
    "path_to_bert = '/datasets/ds_bert/'\n",
    "vocab_file = 'vocab.txt'\n",
    "tokenizer_bert = transformers.BertTokenizer(vocab_file=(path_to_bert + vocab_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Мы будем использовать уже обученную BERT model, которая имеет ограничение на макс кол-во токенов в каждом тексте,\n",
    "поэтому необходимо из имеющегося набора комментариев, как минимум, отобрать только те, длина которых \n",
    "не превышает 512 токенов. Мы сделаем более сильное ограничение на длину текста так, чтобы кол-во токенов примерно \n",
    "соответствовало кол-ву токенов в упражнении тренажера - таким образом мы, во-первых, ускорим работу BERT векторизатора\n",
    "и, во-вторых, максимально приблизим параметры нашей задачи к задаче из тренажера, \n",
    "поскольку не все детали использованного там кода до конца понятны. \"\"\" \n",
    "text_size = df_comments['text'].apply(len)\n",
    "max_text_size = 120\n",
    "df_comments_text_short_only = df_comments[ text_size < max_text_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во комментариев, в каждом из которых не более 120 символов: 50157\n",
      "Доля токсичных комментариев:       15.42%\n"
     ]
    }
   ],
   "source": [
    "# проверим, насколько мы уменьшили первоначальную выборку и не сильно ли изменилось соотношение классов\n",
    "print(f'Кол-во комментариев, в каждом из которых не более {max_text_size} символов:', len(df_comments_text_short_only))\n",
    "print(f'Доля токсичных комментариев:       {df_comments_text_short_only.toxic.mean():6.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# общее кол-во объектов сократилось более чем в 3 раза, доля токсичных комментариев увеличилась\n",
    "# использование даже обученной BERT модели для embeddings требует много времени, поэтому ограничим кол-во объектов\n",
    "limited_no = 15000\n",
    "samples_limited = df_comments_text_short_only.sample(n=limited_no, random_state=RANDOM_STATE)\n",
    "samples_limited = samples_limited.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во комментариев для BERT модели:  15000\n",
      "Доля токсичных комментариев в них:  15.28%\n"
     ]
    }
   ],
   "source": [
    "# проверим, что оставленные объекты не сильно отличаются по балансу классов\n",
    "print('Кол-во комментариев для BERT модели: ', len(samples_limited))\n",
    "print(f'Доля токсичных комментариев в них:  {samples_limited.toxic.mean():6.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.22 s, sys: 42.6 ms, total: 4.27 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# токенизируем комментарии, оставленные для BERT модели,\n",
    "tokenized = samples_limited['text'].apply(lambda x: tokenizer_bert.encode(x, add_special_tokens=True))\n",
    "\n",
    "# дополним 0 все комментарии, менее максимальной длины,\n",
    "max_length = max(len(tokens) for tokens in tokenized)\n",
    "padded = np.array([i + [0]*(max_length - len(i)) for i in tokenized.values])\n",
    "\n",
    "# и создадим маску внимания\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим, достаточно ли мы ограничили длину текста\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим предобученную BERT модель для английского языка\n",
    "config_file = 'bert_config.json'\n",
    "model_file = 'pytorch_model.bin'\n",
    "\n",
    "config_bert = transformers.BertConfig.from_json_file(path_to_bert + config_file)\n",
    "model_bert = transformers.BertModel.from_pretrained(path_to_bert + model_file, config=config_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a41ba1be694f10b7ff98b889b11010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# вычисление embeddings будем производить траншами, обрабатывая 100 комментариев за один транш\n",
    "start_time = time.time()\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model_bert(batch, attention_mask=attention_mask_batch)\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "features = np.concatenate(embeddings)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# время вычисления имбедингов для 15К текстов (из почти 160К) на локальном компьютере составило примерно час\n",
    "# сохраним полученные имбединги в файл, который можно будет использовать в дальнейшем, не тратя время на расчет имбедингов\n",
    "np.save('./features_embeddings_120_15K_text.npy', features)\n",
    "targets = samples_limited['toxic']\n",
    "np.save('./targets_embeddings_120_15K_text.npy', targets)\n",
    "lemmas_time_dict[dict_key] = round(end_time-start_time, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.3.2. Если не рассчитывать имбединги, а загрузить их из локального файла, тогда исполнение кода продолжить отсюда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сначала инициируем строковые переменные, необходимые для заполнения таблицы  all_relults\n",
    "limited_no = 15000\n",
    "max_text_size = 120 \n",
    "name_for_BERT_vectorizer = 'Features preparing: '\n",
    "dict_key = f'BERT tokenizing, {limited_no} texts {max_text_size} length'\n",
    "describe_vectorizing = ', embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# время вычисления имбедингов для 15К текстов (из почти 160К) на локальном компьютере составило примерно час\n",
    "# загрузим полученные ранее имбединги из файла\n",
    "features = np.load('./features_embeddings_120_15K_text.npy')\n",
    "targets = np.load('./targets_embeddings_120_15K_text.npy')\n",
    "# если расчет имбедингов не производится, тогда внесем примерное значение в словарь, где храним время векторизации данных\n",
    "lemmas_time_dict[dict_key] = lemmas_time_dict[lemmas_key] * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=test_size, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=None, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 9, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.026</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 4, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.266</td>\n",
       "      <td>0.028</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, downsampling x 0.5, TfidfVectorizer</th>\n",
       "      <td>0.732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 274, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.759</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.957</td>\n",
       "      <td>106.455</td>\n",
       "      <td>15.418</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 158, n_estimators=1000, num_leaves=70, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.957</td>\n",
       "      <td>136.198</td>\n",
       "      <td>20.005</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, n_estimators=274, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.96</td>\n",
       "      <td>142.534</td>\n",
       "      <td>17.193</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, solver=liblinear, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.939</td>\n",
       "      <td>5.115</td>\n",
       "      <td>0.078</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.732         NaN   \n",
       "LGBMClassifier, best iteration 274, n_estimator...      0.759       0.773   \n",
       "LGBMClassifier, best iteration 158, n_estimator...      0.756       0.776   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...      0.774         NaN   \n",
       "LogisticRegression, solver=liblinear, random_st...      0.784         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.973   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.946   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.886   \n",
       "LGBMClassifier, best iteration 274, n_estimator...       0.849   \n",
       "LGBMClassifier, best iteration 158, n_estimator...       0.875   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...       0.838   \n",
       "LogisticRegression, solver=liblinear, random_st...       0.842   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.936   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            0.957   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            0.957   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...             0.96   \n",
       "LogisticRegression, solver=liblinear, random_st...            0.939   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.885   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.266   \n",
       "LogisticRegression, max_iter=1000, class_weight...           1   \n",
       "LGBMClassifier, best iteration 274, n_estimator...     106.455   \n",
       "LGBMClassifier, best iteration 158, n_estimator...     136.198   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...     142.534   \n",
       "LogisticRegression, solver=liblinear, random_st...       5.115   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.026   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.028   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.012   \n",
       "LGBMClassifier, best iteration 274, n_estimator...          15.418   \n",
       "LGBMClassifier, best iteration 158, n_estimator...          20.005   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...          17.193   \n",
       "LogisticRegression, solver=liblinear, random_st...           0.078   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            57.207   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            57.207   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...            57.207   \n",
       "LogisticRegression, solver=liblinear, random_st...           3432.42   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.301  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.265  \n",
       "LogisticRegression, max_iter=1000, class_weight...                 1.21  \n",
       "LGBMClassifier, best iteration 274, n_estimator...                1.119  \n",
       "LGBMClassifier, best iteration 158, n_estimator...                1.157  \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...                1.083  \n",
       "LogisticRegression, solver=liblinear, random_st...                1.074  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучим линейную регрессию и запишем результаты в общую таблицу\n",
    "fit_and_test(X_train, X_test, y_train, y_test, dict_key, describe_vectorizing, LogisticRegression, solver=solver, random_state=RANDOM_STATE)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's binary_logloss: 0.226409\n"
     ]
    }
   ],
   "source": [
    "# найдем лучшую интерацию для LightGBM\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.4, random_state=RANDOM_STATE)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_STATE)\n",
    "fit_valid_and_test(X_train, X_valid, X_test, y_train, y_valid, y_test, dict_key, describe_vectorizing, lgbm.LGBMClassifier, n_estimators=1000, num_leaves=num_leaves_best, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=None, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 9, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.026</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 4, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.266</td>\n",
       "      <td>0.028</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, downsampling x 0.5, TfidfVectorizer</th>\n",
       "      <td>0.732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 274, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.759</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.957</td>\n",
       "      <td>106.455</td>\n",
       "      <td>15.418</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 158, n_estimators=1000, num_leaves=70, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.957</td>\n",
       "      <td>136.198</td>\n",
       "      <td>20.005</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, n_estimators=274, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.96</td>\n",
       "      <td>142.534</td>\n",
       "      <td>17.193</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, solver=liblinear, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.939</td>\n",
       "      <td>5.115</td>\n",
       "      <td>0.078</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 71, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.658</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.916</td>\n",
       "      <td>11.297</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.732         NaN   \n",
       "LGBMClassifier, best iteration 274, n_estimator...      0.759       0.773   \n",
       "LGBMClassifier, best iteration 158, n_estimator...      0.756       0.776   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...      0.774         NaN   \n",
       "LogisticRegression, solver=liblinear, random_st...      0.784         NaN   \n",
       "LGBMClassifier, best iteration 71, n_estimators...      0.658       0.643   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.973   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.946   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.886   \n",
       "LGBMClassifier, best iteration 274, n_estimator...       0.849   \n",
       "LGBMClassifier, best iteration 158, n_estimator...       0.875   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...       0.838   \n",
       "LogisticRegression, solver=liblinear, random_st...       0.842   \n",
       "LGBMClassifier, best iteration 71, n_estimators...       0.995   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.936   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            0.957   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            0.957   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...             0.96   \n",
       "LogisticRegression, solver=liblinear, random_st...            0.939   \n",
       "LGBMClassifier, best iteration 71, n_estimators...            0.916   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.885   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.266   \n",
       "LogisticRegression, max_iter=1000, class_weight...           1   \n",
       "LGBMClassifier, best iteration 274, n_estimator...     106.455   \n",
       "LGBMClassifier, best iteration 158, n_estimator...     136.198   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...     142.534   \n",
       "LogisticRegression, solver=liblinear, random_st...       5.115   \n",
       "LGBMClassifier, best iteration 71, n_estimators...      11.297   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.026   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.028   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.012   \n",
       "LGBMClassifier, best iteration 274, n_estimator...          15.418   \n",
       "LGBMClassifier, best iteration 158, n_estimator...          20.005   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...          17.193   \n",
       "LogisticRegression, solver=liblinear, random_st...           0.078   \n",
       "LGBMClassifier, best iteration 71, n_estimators...            0.09   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            57.207   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            57.207   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...            57.207   \n",
       "LogisticRegression, solver=liblinear, random_st...           3432.42   \n",
       "LGBMClassifier, best iteration 71, n_estimators...           3432.42   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.301  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.265  \n",
       "LogisticRegression, max_iter=1000, class_weight...                 1.21  \n",
       "LGBMClassifier, best iteration 274, n_estimator...                1.119  \n",
       "LGBMClassifier, best iteration 158, n_estimator...                1.157  \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...                1.083  \n",
       "LogisticRegression, solver=liblinear, random_st...                1.074  \n",
       "LGBMClassifier, best iteration 71, n_estimators...                1.512  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.02</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: no clearing, tokenization, lemmatization WITH pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.941</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.172</td>\n",
       "      <td>739.133</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=None, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 9, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.026</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, upsampling x 4, TfidfVectorizer</th>\n",
       "      <td>0.748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.942</td>\n",
       "      <td>3.266</td>\n",
       "      <td>0.028</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, downsampling x 0.5, TfidfVectorizer</th>\n",
       "      <td>0.732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 274, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.759</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.957</td>\n",
       "      <td>106.455</td>\n",
       "      <td>15.418</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 158, n_estimators=1000, num_leaves=70, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.957</td>\n",
       "      <td>136.198</td>\n",
       "      <td>20.005</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, n_estimators=274, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.96</td>\n",
       "      <td>142.534</td>\n",
       "      <td>17.193</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, solver=liblinear, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.939</td>\n",
       "      <td>5.115</td>\n",
       "      <td>0.078</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 71, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.658</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.916</td>\n",
       "      <td>11.297</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, n_estimators=71, num_leaves=31, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.918</td>\n",
       "      <td>10.162</td>\n",
       "      <td>0.092</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...      0.745         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.746         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.748         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.732         NaN   \n",
       "LGBMClassifier, best iteration 274, n_estimator...      0.759       0.773   \n",
       "LGBMClassifier, best iteration 158, n_estimator...      0.756       0.776   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...      0.774         NaN   \n",
       "LogisticRegression, solver=liblinear, random_st...      0.784         NaN   \n",
       "LGBMClassifier, best iteration 71, n_estimators...      0.658       0.643   \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...      0.672         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.831   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.973   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.946   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.886   \n",
       "LGBMClassifier, best iteration 274, n_estimator...       0.849   \n",
       "LGBMClassifier, best iteration 158, n_estimator...       0.875   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...       0.838   \n",
       "LogisticRegression, solver=liblinear, random_st...       0.842   \n",
       "LGBMClassifier, best iteration 71, n_estimators...       0.995   \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...       0.978   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.941   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.936   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            0.957   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            0.957   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...             0.96   \n",
       "LogisticRegression, solver=liblinear, random_st...            0.939   \n",
       "LGBMClassifier, best iteration 71, n_estimators...            0.916   \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...            0.918   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...       2.131   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.862   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.885   \n",
       "LogisticRegression, max_iter=1000, class_weight...       3.266   \n",
       "LogisticRegression, max_iter=1000, class_weight...           1   \n",
       "LGBMClassifier, best iteration 274, n_estimator...     106.455   \n",
       "LGBMClassifier, best iteration 158, n_estimator...     136.198   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...     142.534   \n",
       "LogisticRegression, solver=liblinear, random_st...       5.115   \n",
       "LGBMClassifier, best iteration 71, n_estimators...      11.297   \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...      10.162   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...            0.02   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.172   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.026   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.028   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.012   \n",
       "LGBMClassifier, best iteration 274, n_estimator...          15.418   \n",
       "LGBMClassifier, best iteration 158, n_estimator...          20.005   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...          17.193   \n",
       "LogisticRegression, solver=liblinear, random_st...           0.078   \n",
       "LGBMClassifier, best iteration 71, n_estimators...            0.09   \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...           0.092   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...           739.133   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            57.207   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            57.207   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...            57.207   \n",
       "LogisticRegression, solver=liblinear, random_st...           3432.42   \n",
       "LGBMClassifier, best iteration 71, n_estimators...           3432.42   \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...           3432.42   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.115  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.114  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.301  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.265  \n",
       "LogisticRegression, max_iter=1000, class_weight...                 1.21  \n",
       "LGBMClassifier, best iteration 274, n_estimator...                1.119  \n",
       "LGBMClassifier, best iteration 158, n_estimator...                1.157  \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...                1.083  \n",
       "LogisticRegression, solver=liblinear, random_st...                1.074  \n",
       "LGBMClassifier, best iteration 71, n_estimators...                1.512  \n",
       "LGBMClassifier, n_estimators=71, num_leaves=31,...                1.455  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iter_lgbm_bert = 71\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.25, random_state=RANDOM_STATE)\n",
    "fit_and_test(X_train, X_test, y_train, y_test, dict_key, describe_vectorizing, lgbm.LGBMClassifier, n_estimators=best_iter_lgbm_bert, num_leaves=num_leaves_best, random_state=RANDOM_STATE)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Выводы.\n",
    "- Использование BERT и корпуса текстов размером в 15000 (в 10 раз меньше, чем исходный корпус) и модели логистической регрессии позволило достигнуть самых высоких результатов по метрике F1 = 0.784.\n",
    "- Использование BERT и корпуса текстов размером в 16000 (в 10 раз меньше, чем исходный корпус) не позволило добиться F1=0.75 для моделей LightGBM.\n",
    "- Надо отметить, что BERT векторизация даже с использованием уже предобученной BERT модели и небольшого корпуса текстов требует значительных ресурсов для векторизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 on test</th>\n",
       "      <th>F1 on valid</th>\n",
       "      <th>F1 on train</th>\n",
       "      <th>Accuracy on test</th>\n",
       "      <th>time to fit</th>\n",
       "      <th>time to predict</th>\n",
       "      <th>time to vectorize</th>\n",
       "      <th>possible overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, solver=liblinear, random_state=134. Features preparing: BERT tokenizing, 15000 texts 120 length, embeddings with BERT conversational, english, cased, 12-layer, 768-hidden, 12-heads, 110M parameters</th>\n",
       "      <td>0.784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.939</td>\n",
       "      <td>5.115</td>\n",
       "      <td>0.078</td>\n",
       "      <td>3432.42</td>\n",
       "      <td>1.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, n_estimators=274, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.96</td>\n",
       "      <td>142.534</td>\n",
       "      <td>17.193</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 274, n_estimators=1000, num_leaves=31, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.759</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.957</td>\n",
       "      <td>106.455</td>\n",
       "      <td>15.418</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier, best iteration 158, n_estimators=1000, num_leaves=70, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length, train+valid+test</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.957</td>\n",
       "      <td>136.198</td>\n",
       "      <td>20.005</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer + text_length</th>\n",
       "      <td>0.752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.943</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression, max_iter=1000, class_weight=balanced, solver=liblinear, random_state=134. Features preparing: clearing, splitting, lemmatization WITHOUT pos-tag, TfidfVectorizer</th>\n",
       "      <td>0.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.942</td>\n",
       "      <td>2.836</td>\n",
       "      <td>0.019</td>\n",
       "      <td>57.207</td>\n",
       "      <td>1.111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   F1 on test F1 on valid  \\\n",
       "LogisticRegression, solver=liblinear, random_st...      0.784         NaN   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...      0.774         NaN   \n",
       "LGBMClassifier, best iteration 274, n_estimator...      0.759       0.773   \n",
       "LGBMClassifier, best iteration 158, n_estimator...      0.756       0.776   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.752         NaN   \n",
       "LogisticRegression, max_iter=1000, class_weight...      0.751         NaN   \n",
       "\n",
       "                                                   F1 on train  \\\n",
       "LogisticRegression, solver=liblinear, random_st...       0.842   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...       0.838   \n",
       "LGBMClassifier, best iteration 274, n_estimator...       0.849   \n",
       "LGBMClassifier, best iteration 158, n_estimator...       0.875   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.833   \n",
       "LogisticRegression, max_iter=1000, class_weight...       0.834   \n",
       "\n",
       "                                                   Accuracy on test  \\\n",
       "LogisticRegression, solver=liblinear, random_st...            0.939   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...             0.96   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            0.957   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            0.957   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.943   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.942   \n",
       "\n",
       "                                                   time to fit  \\\n",
       "LogisticRegression, solver=liblinear, random_st...       5.115   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...     142.534   \n",
       "LGBMClassifier, best iteration 274, n_estimator...     106.455   \n",
       "LGBMClassifier, best iteration 158, n_estimator...     136.198   \n",
       "LogisticRegression, max_iter=1000, class_weight...        3.41   \n",
       "LogisticRegression, max_iter=1000, class_weight...       2.836   \n",
       "\n",
       "                                                   time to predict  \\\n",
       "LogisticRegression, solver=liblinear, random_st...           0.078   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...          17.193   \n",
       "LGBMClassifier, best iteration 274, n_estimator...          15.418   \n",
       "LGBMClassifier, best iteration 158, n_estimator...          20.005   \n",
       "LogisticRegression, max_iter=1000, class_weight...            0.18   \n",
       "LogisticRegression, max_iter=1000, class_weight...           0.019   \n",
       "\n",
       "                                                   time to vectorize  \\\n",
       "LogisticRegression, solver=liblinear, random_st...           3432.42   \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...            57.207   \n",
       "LGBMClassifier, best iteration 274, n_estimator...            57.207   \n",
       "LGBMClassifier, best iteration 158, n_estimator...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "LogisticRegression, max_iter=1000, class_weight...            57.207   \n",
       "\n",
       "                                                   possible overfitting  \n",
       "LogisticRegression, solver=liblinear, random_st...                1.074  \n",
       "LGBMClassifier, n_estimators=274, num_leaves=31...                1.083  \n",
       "LGBMClassifier, best iteration 274, n_estimator...                1.119  \n",
       "LGBMClassifier, best iteration 158, n_estimator...                1.157  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.108  \n",
       "LogisticRegression, max_iter=1000, class_weight...                1.111  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[all_results['F1 on test'] > 0.75].sort_values(by='F1 on test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Наилучшего результата по метрике F1 удалось достигнуть при использовании BERT векторизации, даже сократив используемый для обучения корпус текстов в 10 раз. Метрика F1 на тестовой выборке составили 0.784.\n",
    "- LightGBM при использовании TF-IDF векторизации позволил достичь самой лучшей accuracy и уровня F1 = 0.774. Время, затраченное на обучение и векторизацию/токенизацию/очистку текстов, значительно меньше, чем для BERT.\n",
    "- Самая быстрая модель по времени предобработки текстов и обучения - модель логистической регрессии и TF-IDF векторизации - позволила получить F1 = 0.752."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
